{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work flow for Porto Seguroâ€™s Safe Driver Prediction\n",
    "* Exploratary data analysis\n",
    "* Feature transformation\n",
    "* Different algorithm testing\n",
    "  * LightGBM\n",
    "  * XGB\n",
    "  * CatBoost\n",
    "  * Random Forest\n",
    "  * Regularized Greedy Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amalka\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from rgf.sklearn import RGFClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv('D:\\\\Kaggle Competitions\\\\Porto Seguros Safe Driver Prediction\\\\train.csv',na_values=[-1,-1.0])\n",
    "testData = pd.read_csv('D:\\\\Kaggle Competitions\\\\Porto Seguros Safe Driver Prediction\\\\test.csv',na_values=[-1,-1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging two dataframes in order to easy pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData['dset'] = 'train'\n",
    "testData['dset'] = 'test'\n",
    "combined = pd.concat([trainData, testData], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature modification based on the EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # ps_ind_01 \n",
    "    # mergering but had good roc without merging\n",
    "#combined['ps_ind_01_merged_cat'] = 1\n",
    "#combined.loc[combined.ps_ind_01 < 2 ,'ps_ind_01_merged_cat'] = 0\n",
    "#combined.loc[combined.ps_ind_01 > 2 ,'ps_ind_01_merged_cat'] = 2\n",
    "\n",
    "  # ps_ind_02_cat \n",
    "    # only NA imputation\n",
    "combined['ps_ind_02_cat'].fillna(4,inplace=True)\n",
    "\n",
    "  # ps_ind_03 \n",
    "    # not improved with binning. possible entropy based binning couldnt find a good package\n",
    "#combined['ps_ind_03_merged_cat'] = 1\n",
    "#combined.loc[combined.ps_ind_01 < 2 ,'ps_ind_03_merged_cat'] = 0\n",
    "#combined.loc[combined.ps_ind_01 > 4 ,'ps_ind_03_merged_cat'] = 2\n",
    "\n",
    "  #ps_ind_04_cat\n",
    "combined['ps_ind_04_cat'].fillna(1,inplace=True)\n",
    "\n",
    "  #ps_ind_05_cat : category 5 has lesser count, may be merged to two cat\n",
    "combined['ps_ind_05_cat'].fillna(2,inplace=True)\n",
    "\n",
    "  #ps_ind_06/06/08/09/10/11/12/13/16/17/18_bin \n",
    "    # 10,11,12,13 has very less count on one category\n",
    "    \n",
    "  #ps_ind_14 \n",
    "    # binning possible\n",
    "combined['ps_ind_14_binned'] = combined['ps_ind_14']\n",
    "combined.loc[combined.ps_ind_14 > 0 ,'ps_ind_14_binned'] = 1\n",
    "\n",
    "  #ps_ind_15 \n",
    "    # better roc without any modification\n",
    "#combined['ps_ind_15_binned'] = combined['ps_ind_15']\n",
    "#combined.loc[combined.ps_ind_15 < 3 ,'ps_ind_15_binned'] = 0\n",
    "#combined.loc[combined.ps_ind_15 > 8 ,'ps_ind_15_binned'] = 2\n",
    "#combined.loc[(combined.ps_ind_15 > 2) & (combined.ps_ind_15 < 9) ,'ps_ind_15_binned'] = 1\n",
    "\n",
    "  #ps_reg_01\n",
    "combined['ps_reg_01'] = combined['ps_reg_01']*10\n",
    "\n",
    "  #ps_reg_02\n",
    "combined['ps_reg_02'] = combined['ps_reg_02']*10\n",
    "\n",
    "  #ps_reg_01_02\n",
    "combined['ps_reg_01_02'] = combined['ps_reg_02']*combined['ps_reg_01']    \n",
    "  #ps_reg_03\n",
    "    # NA 8%\n",
    "    # outlier removing\n",
    "combined['ps_reg_03'].fillna(combined['ps_reg_03'].mean(skipna=True),inplace=True)\n",
    "combined['ps_reg_03_mod'] = combined['ps_reg_03']\n",
    "combined.loc[combined.ps_reg_03_mod < 0.25 ,'ps_reg_03_mod'] = 0.25\n",
    "combined.loc[combined.ps_reg_03_mod > 2.25 ,'ps_reg_03_mod'] = 2.25\n",
    "combined['ps_reg_03_mod'] = np.log(combined['ps_reg_03_mod']*100)\n",
    "\n",
    "  #ps_car_01_cat\n",
    "    # NA merged cat 9\n",
    "combined['ps_car_01_cat'].fillna(9,inplace=True)\n",
    "combined.loc[combined.ps_car_01_cat < 4 ,'ps_car_01_cat'] = 3\n",
    "\n",
    "  #ps_car_02_cat\n",
    "    # NA merged cat 9\n",
    "combined['ps_car_02_cat'].fillna(1,inplace=True)\n",
    "\n",
    "  #ps_car_03_cat\n",
    "    # 80% NA\n",
    "\n",
    "  #ps_car_04_cat\n",
    "    # 3,4,5,6,7 has low counts\n",
    "combined.loc[(combined.ps_car_04_cat < 8) & (combined.ps_car_04_cat >2) ,'ps_car_04_cat'] = 3    \n",
    "\n",
    "  #ps_car_05_cat\n",
    "    # 50% NA    \n",
    "combined['ps_car_05_cat'].fillna(2,inplace=True)\n",
    "\n",
    "  #ps_car_06_cat\n",
    "    # 17 categories\n",
    "    # some are having very low count\n",
    "combined.loc[combined.ps_car_06_cat == 5,'ps_car_06_cat'] = 2\n",
    "combined.loc[combined.ps_car_06_cat == 8,'ps_car_06_cat'] = 2\n",
    "combined.loc[combined.ps_car_06_cat == 12,'ps_car_06_cat'] = 2\n",
    "combined.loc[combined.ps_car_06_cat == 13,'ps_car_06_cat'] = 2\n",
    "combined.loc[combined.ps_car_06_cat == 16,'ps_car_06_cat'] = 2\n",
    "combined.loc[combined.ps_car_06_cat == 17,'ps_car_06_cat'] = 2\n",
    "\n",
    "  #ps_car_07_cat\n",
    "    # 2% NA\n",
    "combined['ps_car_07_cat'].fillna(0,inplace=True)\n",
    "combined['ps_car_07_bin'] = combined['ps_car_07_cat'] # renaming to bin\n",
    "\n",
    "  #ps_car_08_cat\n",
    "combined['ps_car_08_bin'] = combined['ps_car_08_cat'] # renaming to bin\n",
    "\n",
    "  #ps_car_09_cat\n",
    "    # NA 569/877\n",
    "    # one category having low count\n",
    "combined['ps_car_09_cat'].fillna(1,inplace=True)\n",
    "combined.loc[combined.ps_car_09_cat == 4,'ps_car_09_cat'] = 1\n",
    "\n",
    "  #ps_car_10_cat\n",
    "    # cat 2 has very low count\n",
    "combined.loc[combined.ps_car_10_cat==2,'ps_car_10_cat'] = 1\n",
    "combined['ps_car_10_bin'] = combined['ps_car_10_cat']\n",
    "\n",
    "  #ps_car_11_cat\n",
    "    # too many categories\n",
    "\n",
    "  #ps_car_11\n",
    "    # better to try with category option\n",
    "combined['ps_car_11'].fillna(1,inplace=True)\n",
    "\n",
    "  #ps_car_12\n",
    "    # NA changed with mean\n",
    "combined['ps_car_12'].fillna(combined['ps_car_12'].mean(skipna=True),inplace=True)\n",
    "combined.loc[combined.ps_car_12>0.75,'ps_car_12'] = 0.75\n",
    "combined.loc[combined.ps_car_12<0.2828,'ps_car_12'] = 0.2828\n",
    "\n",
    "  #ps_car_13\n",
    "combined.loc[combined.ps_car_13>2,'ps_car_13'] = 2\n",
    "\n",
    "  #ps_car_14\n",
    "combined['ps_car_14'].fillna(combined['ps_car_14'].mean(skipna=True),inplace=True)\n",
    "combined.loc[combined.ps_car_14<0.275,'ps_car_14'] <- 0.275\n",
    "combined.loc[combined.ps_car_14>0.575,'ps_car_14'] <- 0.575\n",
    "\n",
    "  #ps_car_15\n",
    "combined['ps_car_15'] = round(combined['ps_car_15']**2,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selected features during EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = [\n",
    "    \"ps_ind_01\",\n",
    "    \"ps_ind_02_cat\", \n",
    "    \"ps_ind_03\",\n",
    "    \"ps_ind_04_cat\",     c\n",
    "    \"ps_ind_05_cat\",      # cat 5 has very low count\n",
    "    \"ps_ind_06_bin\", \n",
    "    \"ps_ind_07_bin\", \n",
    "    \"ps_ind_08_bin\", \n",
    "    \"ps_ind_09_bin\",    \n",
    "    #\"ps_ind_10_bin\",     # non-zero variance \n",
    "    #\"ps_ind_11_bin\",     # non-zero variance \n",
    "    #\"ps_ind_12_bin\",     # non-zero variance \n",
    "    #\"ps_ind_13_bin\",     # non-zero variance\n",
    "    \"ps_ind_14_binned\",  # one category dominates the count, # from the feature importance\n",
    "    \"ps_ind_15\",\n",
    "    \"ps_ind_16_bin\", \n",
    "    \"ps_ind_17_bin\", \n",
    "    \"ps_ind_18_bin\",      \n",
    "    \n",
    "    #\"ps_reg_01\",\n",
    "    #\"ps_reg_02\",\n",
    "    \"ps_reg_01_02\",\n",
    "    #\"ps_reg_03\",\n",
    "    \"ps_reg_03_mod\",\n",
    "    \n",
    "    \"ps_car_01_cat\", \n",
    "    \"ps_car_02_cat\",     \n",
    "    #\"ps_car_03_cat\",     # 80% NA \n",
    "    \"ps_car_04_cat\", \n",
    "    \"ps_car_05_cat\",\n",
    "    \"ps_car_06_cat\",      # saw slight drop in auc. but feature importance is good\n",
    "    \"ps_car_07_bin\",      # rename as bin variable from cat due to two category\n",
    "    \"ps_car_08_bin\",     # rename as bin variable from cat due to two category # from the feature importance\n",
    "    \"ps_car_09_cat\", \n",
    "    \"ps_car_10_bin\",     # merge and rename as bin but accuracy dropped little # from the feature importance\n",
    "    #\"ps_car_11_cat\",     # too many categories when introduce accuracy went down\n",
    "    \"ps_car_11\",         \n",
    "    \"ps_car_12\", \n",
    "    \"ps_car_13\",\n",
    "    \"ps_car_14\",\n",
    "    \"ps_car_15\",\n",
    "    \"ps_calc_01\", \n",
    "    \"ps_calc_02\",\n",
    "    \"ps_calc_03\", \n",
    "    \"ps_calc_04\",\n",
    "    \"ps_calc_05\",        \n",
    "    \"ps_calc_06\",\n",
    "    \"ps_calc_07\", \n",
    "    \"ps_calc_08\",\n",
    "    \"ps_calc_09\", \n",
    "    \"ps_calc_10\",\n",
    "    \"ps_calc_11\", \n",
    "    \"ps_calc_12\",\n",
    "    \"ps_calc_13\",        \n",
    "    \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\",    # from feature importance after introducing calc\n",
    "    \"ps_calc_16_bin\",    # from feature importance after introducing calc\n",
    "    \"ps_calc_17_bin\",    # from feature importance after introducing calc\n",
    "    \"ps_calc_18_bin\",    # from feature importance after introducing calc\n",
    "    \"ps_calc_19_bin\",    # from feature importance after introducing calc\n",
    "    \"ps_calc_20_bin\",    # from feature importance after introducing calc\n",
    "]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preaparing train and test set for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = combined.loc[combined.dset=='test','id'].values\n",
    "target_train = combined.loc[combined.dset=='train','target'].values\n",
    "\n",
    "trainSet = combined.loc[combined.dset=='train',trainFeatures]\n",
    "testSet = combined.loc[combined.dset=='test',trainFeatures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating dummy variables for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [a for a in trainSet.columns if a.endswith('cat')]\n",
    "#for column in cat_features:\n",
    "#    trainSet[column]=trainSet[column].astype('category')\n",
    "#    testSet[column]=testSet[column].astype('category')\n",
    "\n",
    "temp = pd.get_dummies(trainSet[cat_features])\n",
    "trainSet = pd.concat([trainSet,temp],axis=1)\n",
    "trainSet.drop(np.asarray(cat_features),axis=1,inplace=True)\n",
    "\n",
    "temp = pd.get_dummies(testSet[cat_features])\n",
    "testSet = pd.concat([testSet,temp],axis=1)\n",
    "testSet.drop(np.asarray(cat_features),axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62703348  0.62533237]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   52.2s finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "\n",
    "lgb_params = {}\n",
    "lgb_params['n_estimators'] = 500     # n_estimators (int, optional (default=10)) â€“ Number of boosted trees to fit.\n",
    "lgb_params['learning_rate'] = 0.02    # learning_rate (float, optional (default=0.1)) â€“ Boosting learning rate.\n",
    "lgb_params['colsample_bytree'] = 0.8  # colsample_bytree (float, optional (default=1.)) â€“ Subsample ratio of columns when constructing each tree.\n",
    "lgb_params['subsample'] = 0.8         # subsample (float, optional (default=1.)) â€“ Subsample ratio of the training instanc\n",
    "lgb_params['subsample_freq'] = 10     # subsample_freq (int, optional (default=1)) â€“ Frequence of subsample, <=0 means no enable.\n",
    "lgb_params['max_bin'] = 10            # max_bin (int, optional (default=255)) â€“ Number of bucketed bin for feature values.\n",
    "lgb_params['min_child_samples'] = 20  # min_child_samples (int, optional (default=20)) â€“ Minimum number of data need in a child(leaf).\n",
    "lgb_params['random_state'] = 100\n",
    "\n",
    "# Model building\n",
    "lgb_model = LGBMClassifier(**lgb_params)\n",
    "cv_results = cross_val_score(lgb_model, trainSet, target_train, cv=2, scoring='roc_auc',verbose=1)\n",
    "print(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62750036  0.62556041]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.9min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "xgb_params = {}\n",
    "xgb_params['objective'] = 'binary:logistic'\n",
    "xgb_params['learning_rate'] = 0.02\n",
    "xgb_params['n_estimators'] = 1000\n",
    "xgb_params['max_depth'] = 2\n",
    "xgb_params['subsample'] = 0.9\n",
    "xgb_params['colsample_bytree'] = 0.9\n",
    "xgb_params['min_child_weight'] = 10\n",
    "xgb_params['scale_pos_weight'] = 0.04\n",
    "\n",
    "# Model building\n",
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "cv_results = cross_val_score(xgb_model, trainSet, target_train, cv=2, scoring='roc_auc',verbose=1)\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### catBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.61819649  0.61738802]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "#CatBoost params initial\n",
    "cat_params = {}\n",
    "cat_params['iterations'] = 100\n",
    "cat_params['depth'] = 8\n",
    "cat_params['rsm'] = 0.95\n",
    "cat_params['learning_rate'] = 0.03\n",
    "cat_params['l2_leaf_reg'] = 3.5  \n",
    "cat_params['border_count'] = 8\n",
    "cat_params['gradient_iterations'] = 4\n",
    "\n",
    "# Model building\n",
    "catBoost_model = CatBoostClassifier(**cat_params)\n",
    "cv_results = cross_val_score(catBoost_model, trainSet, target_train, cv=2, scoring='roc_auc',verbose=1)\n",
    "print(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57503679  0.57650888]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.0min finished\n"
     ]
    }
   ],
   "source": [
    "rf_params ={}\n",
    "rf_params['n_estimators'] = 100 # The number of trees in the forest.\n",
    "#rf_params['min_samples_split'] = 100  #The minimum number of samples required to split an internal node:\n",
    "rf_params['n_jobs'] = -1\n",
    "\n",
    "# Model building\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "cv_results = cross_val_score(rf_model, trainSet, target_train, cv=2, scoring='roc_auc',verbose=1)\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegularizedGreedyForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6270771   0.62277279]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  9.7min finished\n"
     ]
    }
   ],
   "source": [
    "rgf_params ={}\n",
    "rgf_params['max_leaf'] = 800 \n",
    "rgf_params['algorithm'] = \"RGF_Sib\"\n",
    "rgf_params['test_interval'] = 100\n",
    "\n",
    "\n",
    "# Model building\n",
    "rgf_model = RGFClassifier(**rgf_params)\n",
    "cv_results = cross_val_score(rgf_model, trainSet, target_train, cv=2, scoring='roc_auc',verbose=1)\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
